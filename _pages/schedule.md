---
layout: page
permalink: /schedule/
title: Schedule
description: 
nav: true
nav_order: 4
---


All seminars are happening on Thursday 4-5pm (CET), unless the description specifies a different day/time.


**November 9th - [Elliot Crowley](https://elliotjcrowley.github.io/)** 

Title: Zero-cost proxies and expanding search spaces with program transformations in NAS

Abstract:

Zero-cost proxies are a cheap way of evaluating the performance of an untrained neural network; these allow faster navigation of a search space in NAS which can facilitate the use of more diverse search spaces for a fixed budget. In the first part of this talk, I will review the use of a proxy based on Fisher information for the task of network compression, before motivating the proxy used in our “Neural Architecture Search without Training” work based on measuring the separation of linear regions in a network. In the second part of the talk, I will present a diverse “operation” search space on a convolution that consists of a mix of standard neural operators (e.g. bottlenecking) and program transformations (e.g. loop interchange) and show how zero-cost proxies can be used to produce networks that remain performant but run much faster than their off-the-shelf counterparts.


---------

**November 23rd - [Arkadiy Dushatskiy](https://www.cwi.nl/en/people/arkadiy-dushatskiy/)** 

Title: Multi-Objective Population Based Training Population Based Training

Abstract: 

Population Based Training (PBT) is an efficient hyperparameter optimization algorithm. PBT is a single-objective algorithm, but many real-world hyperparameter optimization problems involve two or more conflicting objectives. In this work, we therefore introduce a multi-objective version of PBT, MO-PBT. Our experiments on diverse multi-objective hyperparameter optimization problems (Precision/Recall, Accuracy/Fairness, Accuracy/Adversarial Robustness) show that MO-PBT outperforms random search, single-objective PBT, and the state-of-the-art multi-objective hyperparameter optimization algorithm MO-ASHA.

---------


**December 7th - TBA** 

Title: TBA

Abstract: TBA

---------
