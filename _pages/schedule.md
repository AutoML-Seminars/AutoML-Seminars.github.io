---
layout: page
permalink: /schedule/
title: Schedule
description: 
nav: true
nav_order: 4
---


All seminars are happening on Thursday 4-5pm (CET), unless the description specifies a different day/time.


----------

**February 2nd <span style="color:red">6pm CET</span> - [Subho Mukherjee](https://www.microsoft.com/en-us/research/people/submukhe/):**

**Title:**

AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers

**Abstract:**

Existing NAS methods operate on a space of dense architectures, where all of the network weights are activated for every input. Motivated by recent advances in sparsely activated models like the Mixture-of-Experts (MoE), we introduce sparse architectures with conditional computation in the NAS search space. Given this expressive search space which subsumes prior dense architectures, we develop a new framework AutoMoE to search for efficient sparse Transformers. AutoMoE-generated sparse models obtain 4x FLOPs reduction and equivalent speedups on CPU over manually designed Transformers with parity in BLEU score on benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute â€“ where different amounts of computations are used for different tokens in the input. AutoMoE code, data and trained models are available at [https://aka.ms/automoe](https://aka.ms/automoe).

----------

**February 16th - Kartik Chandra and Audrey Xie:**

TBA

----------

**March 30th - [Carola Doerr](https://webia.lip6.fr/~doerr/):**

TBA

----------

**April 13th - TBA**

----------

