---
layout: page
permalink: /schedule/
title: Schedule
description: 
nav: true
nav_order: 4
---


Subscribe to our [calendar](https://calendar.google.com/calendar/u/2?cid=YXV0b21sc2VtaW5hckBnbWFpbC5jb20) for the most recent schedule.

---------


**November 20th 3:00pm CET - [Tome Eftimov](http://cs.ijs.si/eftimov/)**

Title: Benchmarking Beyond Statistics: Data-Driven Footprints for Explainable Black-Box Optimization

Abstract: 

This talk explores how emerging benchmarking and meta-learning methodologies are redefining the way we evaluate and select optimization algorithms, moving toward a trustworthy and explainable paradigm. Two promising directions will be highlighted. The first focuses on representative instance selection, ensuring that benchmarking data are diverse and generalizable rather than tailored to narrow or convenient test sets. The second introduces the concept of algorithmic footprints—digital signatures that capture how algorithms interact with problem landscapes, revealing which landscape features influence their success or failure. Together, these developments are paving the way for a new generation of explainable and automated optimization. By replacing simple statistics with interpretable, data-grounded insights, the field is advancing toward a future where black-box optimization becomes as transparent, reproducible, and knowledge-transferable systems.


---------

**November 27th 3:00pm CET - [Jacek Golebiowski](https://www.linkedin.com/in/jacek-golebiowski)**

Title: AutoML without data: training small models with 10 examples

Abstract: 

Organizations need AI that works on their messy data—logs, images, notes, reports—but can't afford to send sensitive information to the cloud. Small Language Models (SLMs) run locally on your own hardware, keeping data private while delivering fast results at a fraction of the cost. Traditionally, building custom AI for production tasks requires expert teams and months of development. AutoML addresses part of this by automating model selection and training, but practitioners still face the bottleneck of manually labeling thousands of training examples. This is why ChatGPT succeeded: it only asks you to describe what you want, not provide labeled datasets. We believe custom models need to match this experience, so in this session we present our model training pipeline that extends AutoML to data preparation itself. You define the problem, and a larger AI "teacher" automatically generates and refines training examples to create a specialized "student" model tailored to tasks like request triage, API chat interfaces, and data transformations. We'll dive deeper into the structure of the generated data and demonstrate how to effectively navigate data generation by controlling the latent variables that define each datapoint.

---------

**Januar 15th 3:00pm CET - [Andrei Paleyes](https://paleyes.info/)**

Title: TBA

Abstract: TBA

---------

**Januar 22nd 3:00pm CET - [Pieter Gijsbers](https://pgijsbers.github.io/)**

Title: TBA

Abstract: TBA

---------





