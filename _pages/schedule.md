---
layout: page
permalink: /schedule/
title: Schedule
description: 
nav: true
nav_order: 4
---


All seminars are happening on Thursday 4-5pm (CET), unless the description specifies a different day/time.

---------

**March 21st - [Arber Zela](https://ml.informatik.uni-freiburg.de/profile/zela/) (University of Freiburg)** 


Title: Multi-objective Differentiable Neural Architecture Search

Abstract: 

Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation.

---------

**March 28th - [Xingyou (Richard) Song](https://xingyousong.github.io/) (Google)** 


Title: OmniPred: Towards Universal Regressors with Language Models 

Abstract: 

Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we introduce OmniPred, a framework for training language models as universal end-to-end regressors over (x, y) evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks at scale, can significantly outperform traditional regression models.



---------

**April 4th - [Sebastian Pineda](https://relea.informatik.uni-freiburg.de/people/sebastian-pineda)** 


Title: 

Abstract: 


---------

**April 18th - [Julien Siems](https://scholar.google.de/citations?user=rKgTTh8AAAAJ&hl=de) and [Riccardo Grazzi](https://scholar.google.de/citations?user=9Tlyx1IAAAAJ&hl=de)** 


Title: 

Abstract: 

---------



