---
layout: page
permalink: /schedule/
title: Schedule
description: 
nav: true
nav_order: 4
---


All seminars are happening on Thursday 4-5pm (CET), unless the description specifies a different day/time.


---------

**September 21st - [Jean Kaddour](https://jeankaddour.com/) and [Oscar Key](https://oscarkey.github.io/)** 

Title: No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models 

Abstract: 

The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/jeankaddour/notrainnogain.

---------

**October 5th - [Noah Hollmann](https://ml.informatik.uni-freiburg.de/profile/hollmann/) and [Samuel MÃ¼ller](https://samuelgabriel.github.io/index.pdf)** 

Title: LLMs for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering

Abstract: 

As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature. CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML.

---------

**October 19th - TBA** 

Title: TBA

Abstract: TBA

---------

**November 9th - [Elliot Crowley](https://elliotjcrowley.github.io/)** 

Title: TBA

Abstract: TBA

---------

**November 23rd - [Arkadiy Dushatskiy](https://www.cwi.nl/en/people/arkadiy-dushatskiy/)** 

Title: Multi-Objective Population Based Training Population Based Training

Abstract: 

Population Based Training (PBT) is an efficient hyperparameter optimization algorithm. PBT is a single-objective algorithm, but many real-world hyperparameter optimization problems involve two or more conflicting objectives. In this work, we therefore introduce a multi-objective version of PBT, MO-PBT. Our experiments on diverse multi-objective hyperparameter optimization problems (Precision/Recall, Accuracy/Fairness, Accuracy/Adversarial Robustness) show that MO-PBT outperforms random search, single-objective PBT, and the state-of-the-art multi-objective hyperparameter optimization algorithm MO-ASHA.

---------


**December 7th - TBA** 

Title: TBA

Abstract: TBA

---------
